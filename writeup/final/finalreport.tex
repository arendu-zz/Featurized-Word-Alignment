%
% File naaclhlt2010.tex
%
% Contact: nasmith@cs.cmu.edu

\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2010}
\usepackage{times}
\usepackage{latexsym}
\setlength\titlebox{6.5cm}    % Expanding the titlebox
\usepackage{amsmath,amsfonts,amsthm}
\usepackage{amssymb}
\usepackage[colorlinks=true]{hyperref}
\usepackage{makecell}

\title{Unsupervised Featureized HMM Modeling}

\author{Adithya Renduchintala
  \quad\quad Shuoyang Ding\\
  Johns Hopkins University\\
  3400 North Charles Street\\
  Baltimore, MD 21218, USA\\
  {\tt \{adithya.renduchintala, dings\}@jhu.edu}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  
\end{abstract}

\section{Introduction}
\cite{brown1993mathematics}
\cite{vogel1996hmm}
\cite{berg2010painless}
\cite{baum1970maximization}

\section{Featurized HMM}
\subsection{Fully Observed}
Before deriving the EM algorithm applied to HMMs (Baum-Welch), we will first
look at the case where both the observation variables and the hidden variables
are fully observed.\\
Consider a observation sequence $X = \{x_1, x_2, \ldots x_n \}$ with an associated sequence of states $S = \{s_1, s_2, \ldots s_n\}$ the probability
of this sequence under the HMM model is:
\begin{align}
P(X \mid \theta) &= \prod_{i=1}^{|X|} a(s_i \mid s_{i-1}) e(x_i \mid
s_i)\label{eq1}
\end{align}
% \todo[inline, color=green!40]{Presentational note: it's good practice to introduce your notation gently at the beginning, e.g. here you should also introduce $S$, $a$, and $e$.}
Where $a()$ and $e()$ represent the state transition and emission probabilities.
We can re-write the above expression in terms of the number of times we have
seen a particular transition and emission, and their associated conditional
probability i.e $a(s_i = l \mid s_{i-1} = k)$ and $e(x_i = x_b \mid s_i = l)$
Suppose we have seen the transition $ s_{i-1} = 
\rightarrow s_i = l $,
$A_{kl}$ times and the emission $s_i = l \rightarrow x_i = x_b$, $E_{lb}$ times,
then:\\
% \todo[inline, color=green!40]{Presentational note: it often helps the reader if you use notation in a consistent way, e.g. all r.v.'s are capital letters and constants are lowercase letters, or vice versa. Here you have $X$ as a r.v. but $k$ and $l$ are constant.}
\begin{align}
P(X \mid \theta) &= \prod_{ k,l \in \mathbf{S,S}} a(s_l \mid s_k)^{A_{kl}}
\prod_{ x_b,L \in \mathbf{V,S}}e(x_b \mid s_l)^{E_{lb}}\label{eq2}
\end{align}
Where $\mathbf{S}$ is the set of all possible hidden states and $\mathbf{V}$ is
the possible observation tokens.\
In the fully observed case, we can simply count $A_{kl}$ and $E_{lb}$, then to
maximize the probability of the data under the model we simple find the best
$a(s_l \mid s_k)$ and $(x_i = x_b \mid s_i = l)$ using MLE by applying the
constraints ensuring that the 2 parameters are probabilities.
% \todo[color=green!40]{Not necessary here, but some time try to derive this using Lagrange multipliers.} This gives us:
\begin{align*}
a(s_l \mid s_k) &= \frac{A_{kl}}{\sum_{l'} A_{kl'}}\\
e(x_b \mid s_l) &=
\frac{E_{lb}}{\sum_{b'} E_{lb'}}\\
\end{align*}

\subsection{Hidden States}
Now in this case, we do not know $A_{kl}$ and $E_{lb}$. Since the corresponding
state sequence $S$ is unknown, to find $P(X\mid \theta)$ we must marginalize
over all possible state sequences. Then the Expected counts of $A_{kl}$ and
$E_{lb'}$ are the weighted average of the occurrences of the events
$K \rightarrow L$ and $L \rightarrow x_b$ for each $S' \in possible sequences$.
\begin{align}
A_{kl} &= \sum_{S'} A^{S'}_{kl} P(S' \mid X, \theta)\label{eq:akl}\\
E_{lb} &= \sum_{S'} E^{S'}_{lb} P(S' \mid X, \theta)\label{eq:elb}
\end{align}
Thus, the counting process is converted to an averaging process. Then the
probability of the observation under the model still remains the same as
\eqref{eq2}, except the known counts are expected counts. Obviously computing
the sums over all possible states, by explicitly iterating over all state sequences, is not possible
% \todo[color=green!40]{But it is! As you show next.}.
To do with efficiently we use the
forward-backward algorithm. The expressions \eqref{eq:akl} and \eqref{eq:elb}
can be written as
\begin{align}
A_{kl} &= \frac{1}{P(X|\theta)} \sum_i^{\mid X \mid}
F_k(i-1)a(s_l|s_k)e(x_b|s_l) B_l(i) \label{eq:aklfb}\\
E_{lb} &= \frac{1}{P(X\mid \theta)} \sum_{i,x_i == x_b}^{\mid X \mid}
F_l(i)e(x_b  \mid s_l)B_l(i) \label{eq:elbfb}
\end{align}
This can be derived from realizing that 
\begin{align*}
& P(s_{i-1} = k, s_i = l \mid \theta) = \\
& \quad\quad F_k(i-1)a(s_l \mid
s_k)e(x_i \mid s_l)B_l(i)\\
& F_k(i-1) \text{: forward probability of $(i-1)^{th}$ iteration}\\ &\quad\quad \text{at state $s_k$}\\
& a(s_l \mid s_k) \text{: transition probability from state $s_k \rightarrow s_l$}\\
& e(x_i \mid s_l) \text{: emission probability from state $s_l$ to}\\ &\quad\quad \text{observation $x_i$}\\
& B_l(i) \text{: backward probability to $i^{th}$ time iteration at}\\ &\quad\quad \text{state $s_l$}
\end{align*}
Substituting \eqref{eq:aklfb} and \eqref{eq:elbfb} in \eqref{eq2} and taking
the log we get the log-likelihood of the data under the model in terms of
expected counts and initial parameters $\theta$
\begin{align} \begin{split}
log(P(X\mid \theta)) =& \sum_{s_k \rightarrow s_l} A_{kl} log (a'(s_l \mid
s_k)) +\\
&  \sum_{s_l \rightarrow x_b} E_{lb} log( e'(x_i = x_b \mid s_l))
\label{eq:log}\\
\end{split} 
\end{align}
In the above expression the parameters $a'$ and $e'$ are from $\theta'$ which is
what we want to set such that the log likelihood of the data is maximized. These
are the parameters (conditional probabilities) that can be modeled as a
log-linear model. This is where the featurized HMM deviated from the typical
Baum-Welch algorithm. In the typical case, the new values of the parameters that
maximized the data likelihood can simply be computed by renormalizing the the
expected counts for example the update the transition parameters in Baum-Welch
we do $a'(s_l \mid s_k) = \frac{A_{kl}} {\sum_{L'} A_{kl'}}$.
\begin{align}
a'(s_l \mid s_k) &= \frac{exp(\theta' \cdot f(s_k, s_l))}{\sum_{s_l'}
exp(\theta' \cdot f(s_k,s_l'))} \label{eq:probakl}\\
e'(x_b \mid s_l) &= \frac{exp(\theta' \cdot
f(s_l, x_b))}{\sum_{x_b'} exp(\theta' \cdot f(s_l, x_b'))} \label{eq:probelb}
\end{align}
substituting in \eqref{eq:log} we get
\begin{align}
\begin{split}
L(X,\theta) =& \sum_{s_k \rightarrow s_l} A_{kl} (\theta' \cdot f(s_k, s_l) -
log \sum_{s_l'} exp( \theta' \cdot f(s_k, s_l'))) + \\
& \sum_{s_l \rightarrow x_b} E_{lb} (\theta' \cdot f(x_b, s_l) - log\sum_{x_b'}
exp(\theta' \cdot f(s_l, x_b'))) \label{eq:logsub}
\end{split}
\end{align}
Here $f$ is a binary feature function and $\theta'$ are the weights of the
features that we want to optimize such that the likelihood of the data is
maximized. It should be noted here that the expected counts are with respect to
the initial $\theta$ chosen arbitrarily (usually with a uniform distribution)
while the conditional probabilities (modeled as log-linear parameters) are with
respect to the new $\theta'$. To obtain the gradient of the this log-likelihood
function we take the derivative of $\eqref{eq:log}$ with respect to each
$\theta_j \in \theta$.
\begin{align}
\begin{split}
\frac{\partial}{\partial\theta_j} L(X,\theta) =&  \sum_{s_k \rightarrow s_l} A_{kl}
(f_j(s_k, s_l) - \sum_{s_l'} f_j(s_k,s_l') \cdot a(s_l\mid a_k)) +\\
& \sum_{s_l \rightarrow x_b} E_{lb}(f_j(s_k, s_l) - \sum_{x_b'} f_j(s_k,s_l')
\cdot e(x_b' \mid s_l)) \label{eq:grad}
\end{split}
\end{align}

\section{Implementations}
% write about implementation issues you encountered
% make sure to clarify all the different optimization method you've used

\section{Experiments}
\subsection{setup}
Most of our word alignment experiments are performed on a parallel corpus of English and Spanish sentences.
The corpus contains 1768 sentences as the training set and 200 sentences as the development set.
As there are not tuning hyper-parameters in our model, it is sensible and sufficient to evaluate the performance of our model on the development set.
For several experiments, we also use an extra training corpus that contains 5K % TODO what is the exact scale of the 5k dataset?
sentences.

As we have mentioned in the previous part, we take traditional IBM Model1 as our baseline.
% make sure we have mentioned this in the previous parts of our writeup
For featurized model, we designed three feature templates (as shown in Table \ref{featureset-table}) to try out on it.
Our general workflow is to first prove that the featurized model is able to generate a comparable result against baseline when we only use \texttt{BASE} features (because they are equivalent).
After that, we further incorporate other additional features to see how they works.

We would also like to see how different factors (e.g. optimization algorithm, iteration, regularization etc.) will effect the performance of the model.

\begin{table}
\begin{center}
\begin{tabular}{|c|c|}
\hline 
\bf template & \bf description \\
\hline
\texttt{BASE} & \makecell[l]{binary feature, fires for each\\ source and target pairs in the\\ alignment} \\
\texttt{IS\_SAME} & \makecell[l]{binary feature, fires when\\ the source and the target token\\ is same}\\
\texttt{EDIT\_DIST} & \makecell[l]{real-valued feature, gives the\\ edit distance between the source\\ and target token}\\
\hline
\end{tabular}
\end{center}
\caption{\label{featureset-table} Feature templates for experiments}
\end{table}

\subsection{Results}
\begin{table*}[t]
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline \bf  & \bf Precision & \bf Recall & \bf F-score \\ \hline
\texttt{BASELINE} & 0.374 & 0.384 & 0.379 \\
% this is a little bit higher than the result I gave you
% because I used more iterations and this is the converged result
\hline
\hline
\texttt{EM\_DFLTITER\_RC0.0\_2K\_BASE} & 0.374 & 0.381 & 0.377 \\
\texttt{EM\_DFLTITER\_RC0.005\_2K\_BASE} & 0.357 & 0.364 & 0.360 \\
\hline
\end{tabular}
\end{center}
\caption{\label{font-table} Font guide. }
\end{table*}

\section{Comparison with Proposal}
\bibliographystyle{naaclhlt2010}
\bibliography{finalbib}

\end{document}
