%
% File naaclhlt2010.tex
%
% Contact: nasmith@cs.cmu.edu

\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2010}
\usepackage{times}
\usepackage{latexsym}
\setlength\titlebox{6.5cm}    % Expanding the titlebox
\usepackage{amsmath,amsfonts,amsthm}
\usepackage{amssymb}

\title{Unsupervised Featureized HMM Modeling}

\author{Adithya Renduchintala
  \quad\quad Shuoyang Ding\\
  Johns Hopkins University\\
  3400 North Charles Street\\
  Baltimore, MD 21218, USA\\
  {\tt \{adithya.renduchintala, dings\}@jhu.edu}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  
\end{abstract}

\section{Introduction}
\cite{brown1993mathematics}
\cite{vogel1996hmm}
\cite{berg2010painless}
\cite{baum1970maximization}

\section{Featurized HMM}
\subsection{Fully Observed}
Before deriving the EM algorithm applied to HMMs (Baum-Welch), we will first
look at the case where both the observation variables and the hidden variables
are fully observed.\\
Consider a observation sequence $X = \{x_1, x_2, \ldots x_n \}$ with an associated sequence of states $S = \{s_1, s_2, \ldots s_n\}$ the probability
of this sequence under the HMM model is:
\begin{align}
P(X \mid \theta) &= \prod_{i=1}^{|X|} a(s_i \mid s_{i-1}) e(x_i \mid
s_i)\label{eq1}
\end{align}
% \todo[inline, color=green!40]{Presentational note: it's good practice to introduce your notation gently at the beginning, e.g. here you should also introduce $S$, $a$, and $e$.}
Where $a()$ and $e()$ represent the state transition and emission probabilities.
We can re-write the above expression in terms of the number of times we have
seen a particular transition and emission, and their associated conditional
probability i.e $a(s_i = l \mid s_{i-1} = k)$ and $e(x_i = x_b \mid s_i = l)$
Suppose we have seen the transition $ s_{i-1} = 
\rightarrow s_i = l $,
$A_{kl}$ times and the emission $s_i = l \rightarrow x_i = x_b$, $E_{lb}$ times,
then:\\
% \todo[inline, color=green!40]{Presentational note: it often helps the reader if you use notation in a consistent way, e.g. all r.v.'s are capital letters and constants are lowercase letters, or vice versa. Here you have $X$ as a r.v. but $k$ and $l$ are constant.}
\begin{align}
P(X \mid \theta) &= \prod_{ k,l \in \mathbf{S,S}} a(s_l \mid s_k)^{A_{kl}}
\prod_{ x_b,L \in \mathbf{V,S}}e(x_b \mid s_l)^{E_{lb}}\label{eq2}
\end{align}
Where $\mathbf{S}$ is the set of all possible hidden states and $\mathbf{V}$ is
the possible observation tokens.\
In the fully observed case, we can simply count $A_{kl}$ and $E_{lb}$, then to
maximize the probability of the data under the model we simple find the best
$a(s_l \mid s_k)$ and $(x_i = x_b \mid s_i = l)$ using MLE by applying the
constraints ensuring that the 2 parameters are probabilities.
% \todo[color=green!40]{Not necessary here, but some time try to derive this using Lagrange multipliers.} This gives us:
\begin{align*}
a(s_l \mid s_k) &= \frac{A_{kl}}{\sum_{l'} A_{kl'}}\\
e(x_b \mid s_l) &=
\frac{E_{lb}}{\sum_{b'} E_{lb'}}\\
\end{align*}

\subsection{Hidden States}
Now in this case, we do not know $A_{kl}$ and $E_{lb}$. Since the corresponding
state sequence $S$ is unknown, to find $P(X\mid \theta)$ we must marginalize
over all possible state sequences. Then the Expected counts of $A_{kl}$ and
$E_{lb'}$ are the weighted average of the occurrences of the events
$K \rightarrow L$ and $L \rightarrow x_b$ for each $S' \in possible sequences$.
\begin{align}
A_{kl} &= \sum_{S'} A^{S'}_{kl} P(S' \mid X, \theta)\label{eq:akl}\\
E_{lb} &= \sum_{S'} E^{S'}_{lb} P(S' \mid X, \theta)\label{eq:elb}
\end{align}
Thus, the counting process is converted to an averaging process. Then the
probability of the observation under the model still remains the same as
\eqref{eq2}, except the known counts are expected counts. Obviously computing
the sums over all possible states, by explicitly iterating over all state sequences, is not possible
% \todo[color=green!40]{But it is! As you show next.}.
To do with efficiently we use the
forward-backward algorithm. The expressions \eqref{eq:akl} and \eqref{eq:elb}
can be written as
\begin{align}
A_{kl} &= \frac{1}{P(X|\theta)} \sum_i^{\mid X \mid}
F_k(i-1)a(s_l|s_k)e(x_b|s_l) B_l(i) \label{eq:aklfb}\\
E_{lb} &= \frac{1}{P(X\mid \theta)} \sum_{i,x_i == x_b}^{\mid X \mid}
F_l(i)e(x_b  \mid s_l)B_l(i) \label{eq:elbfb}
\end{align}
This can be derived from realizing that 
\begin{align*}
& P(s_{i-1} = k, s_i = l \mid \theta) = \\
& \quad\quad F_k(i-1)a(s_l \mid
s_k)e(x_i \mid s_l)B_l(i)\\
& F_k(i-1) \text{: forward probability of $(i-1)^{th}$ iteration}\\ &\quad\quad \text{at state $s_k$}\\
& a(s_l \mid s_k) \text{: transition probability from state $s_k \rightarrow s_l$}\\
& e(x_i \mid s_l) \text{: emission probability from state $s_l$ to}\\ &\quad\quad \text{observation $x_i$}\\
& B_l(i) \text{: backward probability to $i^{th}$ time iteration at}\\ &\quad\quad \text{state $s_l$}
\end{align*}
Substituting \eqref{eq:aklfb} and \eqref{eq:elbfb} in \eqref{eq2} and taking
the log we get the log-likelihood of the data under the model in terms of
expected counts and initial parameters $\theta$
\begin{align} \begin{split}
log(P(X\mid \theta)) =& \sum_{s_k \rightarrow s_l} A_{kl} log (a'(s_l \mid
s_k)) +\\
&  \sum_{s_l \rightarrow x_b} E_{lb} log( e'(x_i = x_b \mid s_l))
\label{eq:log}\\
\end{split} 
\end{align}
In the above expression the parameters $a'$ and $e'$ are from $\theta'$ which is
what we want to set such that the log likelihood of the data is maximized. These
are the parameters (conditional probabilities) that can be modeled as a
log-linear model. This is where the featurized HMM deviated from the typical
Baum-Welch algorithm. In the typical case, the new values of the parameters that
maximized the data likelihood can simply be computed by renormalizing the the
expected counts for example the update the transition parameters in Baum-Welch
we do $a'(s_l \mid s_k) = \frac{A_{kl}} {\sum_{L'} A_{kl'}}$.
\begin{align}
a'(s_l \mid s_k) &= \frac{exp(\theta' \cdot f(s_k, s_l))}{\sum_{s_l'}
exp(\theta' \cdot f(s_k,s_l'))} \label{eq:probakl}\\
e'(x_b \mid s_l) &= \frac{exp(\theta' \cdot
f(s_l, x_b))}{\sum_{x_b'} exp(\theta' \cdot f(s_l, x_b'))} \label{eq:probelb}
\end{align}
substituting in \eqref{eq:log} we get
\begin{align}
\begin{split}
L(X,\theta) =& \sum_{s_k \rightarrow s_l} A_{kl} (\theta' \cdot f(s_k, s_l) -
log \sum_{s_l'} exp( \theta' \cdot f(s_k, s_l'))) + \\
& \sum_{s_l \rightarrow x_b} E_{lb} (\theta' \cdot f(x_b, s_l) - log\sum_{x_b'}
exp(\theta' \cdot f(s_l, x_b'))) \label{eq:logsub}
\end{split}
\end{align}
Here $f$ is a binary feature function and $\theta'$ are the weights of the
features that we want to optimize such that the likelihood of the data is
maximized. It should be noted here that the expected counts are with respect to
the initial $\theta$ chosen arbitrarily (usually with a uniform distribution)
while the conditional probabilities (modeled as log-linear parameters) are with
respect to the new $\theta'$. To obtain the gradient of the this log-likelihood
function we take the derivative of $\eqref{eq:log}$ with respect to each
$\theta_j \in \theta$.
\begin{align}
\begin{split}
\frac{\partial}{\partial\theta_j} L(X,\theta) =&  \sum_{s_k \rightarrow s_l} A_{kl}
(f_j(s_k, s_l) - \sum_{s_l'} f_j(s_k,s_l') \cdot a(s_l\mid a_k)) +\\
& \sum_{s_l \rightarrow x_b} E_{lb}(f_j(s_k, s_l) - \sum_{x_b'} f_j(s_k,s_l')
\cdot e(x_b' \mid s_l)) \label{eq:grad}
\end{split}
\end{align}

\section{Implementations}
% write about implementation issues you encountered
% make sure to clarify all the different optimization method you've used

\section{Experiments}


\section{Comparison with Proposal}
\bibliographystyle{naaclhlt2010}
\bibliography{finalbib}

\end{document}
